<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>JS Transformer Playground + Local Tokenizer Upload + GGUF Loader</title>
<style>
        :root {
          --main-accent: #3191e0;
          --main-bg: #181d24;
          --main-text: #cfeafd;
          --panel-bg: #121820;
        }
        body { 
            font-family: monospace; 
            background: var(--main-bg); 
            color: var(--main-text); 
            padding: 24px; 
        }
        .output { margin-top: 24px; font-size: 1.1em; color: var(--main-accent);}
         button.btn,
         input.btn,
         .btn {
            background: var(--main-accent);
            color: #fff !important;
            border: none;
            padding: 8px 16px;
            cursor: pointer;
            margin-top: 12px;
            border-radius: 3px;
            transition: background 0.15s, color 0.15s;
            font: inherit;
         }

         button.btn:hover,
         input.btn:hover,
         .btn:hover,
         button.btn:focus,
         input.btn:focus,
         .btn:focus,
         button.btn:active,
         input.btn:active,
        .btn:active {
           background: #1567b7;
           color: #fff !important;
           outline: none;
        }
        input[type="number"], input[type="text"], textarea { 
            background: #222c38; color: var(--main-text); border: 1px solid #314f75; 
            border-radius: 4px;
        }
        .fileUpload { margin: 18px 0 8px 0; }
        .tensorList { 
            max-height: 320px; 
            overflow: auto; 
            background: var(--panel-bg); 
            font-size: 0.98em; 
            margin: 0.5em 0 0.5em 0; 
            padding: 6px 10px;
            border-radius: 8px;
            color: var(--main-accent);
        }
        .inputVecBox { 
            width: 76%; 
            margin: 0.8em 0; 
            padding: 4px; 
            font-size: 1em; 
            background: #202c38; 
            color: var(--main-text);
            border: 1px solid #314f75;
            border-radius: 4px;
        }
        #localTokOutput { margin-left: 1em; color: var(--main-accent); }
        a, a:visited { color: #86bff7; }
        h1, h2, h3 { color: var(--main-accent);}
        [style*="color:#4f4"], [style*="color:#ffa"], [style*="color:#bffcff"], [style*="color:#4af"], [style*="color:#f4a"], [style*="color:#ffe138"], [style*="color:#ffa"] {
            color: var(--main-accent)!important;
        }
        [style*="border-left: 4px solid"], [style*="border-left: 3px solid"], [style*="border: 3px solid"], [style*="border: 2px solid"], [style*="border:2px solid"] {
            border-color: var(--main-accent)!important;
        }
        [style*="background: #1a3a1a"], [style*="background: #1a2a3a"],[style*="background: #2a1a2a"],[style*="background: #1a2a1a"],[style*="background: #2a2a2a"],[style*="background: #1a1a1a"],[style*="background: #2a2a2a"],[style*="background: #2a2a2a"] {
            background: var(--panel-bg)!important;
        }
        .tensorList b,
        summary,
        label,
        .btn,
        .output,
        #localTokOutput {
            color: var(--main-accent)!important;
        }
        details[open] summary {
            border-bottom: 1px solid #2c4d72;
        }
        ::-webkit-scrollbar {
          width: 8px;
        }
        ::-webkit-scrollbar-thumb {
          background: #223957;
          border-radius: 8px;
        }
        ::selection {
          background: var(--main-accent); 
          color: #fff;
        }
        [style*="color:#f885"], [style*="color:#f84"], [style*="color:#f44"] {
            color: #ec197b!important;
        }
    </style>
<body>
<h1>ü§ñ Browser Transformer: GGUF + Text Generation</h1>
<div style="background: #1a3a1a; border-left: 4px solid #4f4; padding: 16px; margin: 16px 0; border-radius: 4px;">
    <h2 style="margin-top:0; color:#4f4;">üìñ What is this?</h2>
    <p style="margin: 8px 0;">A <b>complete transformer implementation</b> written in vanilla JavaScript that runs entirely in your browser. No server, no Python, no frameworks.</p>
    <p style="margin: 8px 0;"><b>Features:</b> GGUF file parser ‚Ä¢ Full weight extraction (all 149 tensors) ‚Ä¢ Multi-head attention ‚Ä¢ Feed-forward networks ‚Ä¢ Autoregressive text generation</p>
    <p style="margin: 8px 0; color:#ffa;"><b>‚ö° NEW:</b> Now loads REAL GPT-2 weights from GGUF files!</p>
</div>

<!-- ============ SECTION 1: TEST PLAYGROUND (ADVANCED) ============ -->
<details style="border: 2px solid #444; padding: 16px; margin: 24px 0; border-radius: 8px; background: #2a2a2a;">
    <summary style="cursor: pointer; font-size: 1.2em; font-weight: bold; color: #88f;">
        üî¨ Advanced: Raw Transformer Playground (Custom Vectors)
    </summary>
    <div style="margin-top: 16px; padding: 12px; background: #1a1a1a; border-radius: 4px;">
        <p style="color: #aaa; margin: 8px 0;">This section lets you test the transformer with <b>custom numerical vectors</b>. This is for understanding the raw math - most users should skip this and use the text generation below.</p>
    </div>
    <div style="margin-top: 16px;">
        <label>Sequence length: <input type="number" id="seqLen" value="10" min="1" max="64"></label><br>
        <label>Embedding dimension: <input type="number" id="embedDim" value="16" min="4" max="128"></label><br>
        <label>Num heads: <input type="number" id="numHeads" value="2" min="1" max="8"></label><br>
        <label>Blocks: <input type="number" id="numBlocks" value="2" min="1" max="8"></label><br>
        <label>Hidden (FFN) dim: <input type="number" id="ffnHidden" value="32" min="4" max="256"></label><br>
        <label>Classes: <input type="number" id="numClasses" value="3" min="1" max="10000"></label><br>
    </div>
    <div style="margin-top:16px">
        <b>Input vector (comma-separated numbers, must match embedding dimension):</b><br>
        <textarea id="inputVec" class="inputVecBox" rows="2" placeholder="e.g. 0.12,0.0,1.23,..."></textarea>
    </div>
    <button class="btn" onclick="runTransformer()">Run Transformer Prediction</button>
    <div class="output" id="output"></div>
</details>
<!-- ============ SECTION 2: GGUF LOADER ============ -->
<div style="border: 2px solid #4af; padding: 16px; margin: 24px 0; border-radius: 8px; background: #1a2a3a;">
    <h2 style="margin-top:0; color:#4af;"> Step 1: Load GGUF Model File</h2>
    <p style="color: #aaa; margin: 8px 0;">Upload a GGUF model file (like GPT-2-f32.gguf) to extract ALL weights. This parses the file and loads all 149 tensors.</p>
    <div class="fileUpload">
        <label style="font-weight:bold">
            GGUF Model File:
            <input type="file" id="ggufFileInput" accept=".gguf">
        </label>
        <button id="loadGgufBtn" class="btn" style="margin-left:10px">Load GGUF</button>
        <span id="ggufStatus" style="margin-left:14px; color:#bffcff"></span>
    </div>
    <div id="tensorListPanel"></div>
    <div id="weightsStatus" style="margin-top:12px;"></div>
</div>

<!-- ============ SECTION 3: TOKENIZER LOADER ============ -->
<div style="border: 2px solid #f4a; padding: 16px; margin: 24px 0; border-radius: 8px; background: #2a1a2a;">
    <h2 style="margin-top:0; color:#f4a;"> Step 2: Load Tokenizer</h2>
    <p style="color: #aaa; margin: 8px 0;">Upload the tokenizer.json file (from the same model) to convert text into token IDs.</p>
    <input type="file" id="localTokenizerInput" accept=".json" />
    <button class="btn" onclick="loadLocalTokenizer()">Load Tokenizer</button>
    <span id="localTokOutput"></span>
    
    <details style="margin-top: 16px; background: #1a1a1a; padding: 12px; border-radius: 4px;">
        <summary style="cursor: pointer; color: #aaa;"> Test Tokenizer (Optional)</summary>
        <div style="margin-top: 12px;">
            <p style="color: #888; font-size: 0.9em;">This is just for testing - type text to see how it's tokenized.</p>
            <input type="text" id="userText" style="width:70%;margin-top: 0.6em" placeholder="Type something to test...">
            <button class="btn" onclick="encodeTextLocal()">Encode Text</button>
            <div id="tokencodeOutput" style="margin:0.5em 0 0 0;"></div>
        </div>
    </details>
</div>

<!-- ============ SECTION 4: TEXT GENERATION ============ -->
<div style="border: 3px solid #4f4; padding: 20px; margin: 24px 0; border-radius: 8px; background: #1a2a1a;">
    <h2 style="margin-top:0; color:#4f4;">üöÄ Step 3: Generate Text!</h2>
    <p style="color: #aaa; margin: 8px 0; font-size: 1.1em;">Now that you've loaded the GGUF weights and tokenizer, you can generate text with REAL GPT-2 weights!</p>
    <textarea id="promptText" style="width:90%;margin-top: 0.8em; height: 80px; padding: 8px; font-size: 1em;" placeholder="Enter your prompt here... (e.g., 'Once upon a time')">Once upon a time</textarea><br>
    <label style="margin-top: 12px; display: block;">Max new tokens: <input type="number" id="maxTokens" value="5" min="1" max="100" style="width: 60px;"></label>
    <button class="btn" onclick="generateText()" style="margin-top: 12px; padding: 12px 24px; font-size: 1.1em; background: #4f4; color: #000;">Generate Text!</button>
    <div id="generationOutput" style="margin:1em 0 0 0;"></div>
</div>
<script>
// =================== GGUF TENSOR STORAGE ===================
let ggufTensors = {};        // All extracted tensors by name
let ggufBuffer = null;       // Raw ArrayBuffer for reading tensor data
let tensorDataStart = 0;     // Where tensor data begins in the file
let embeddingMatrix = null;  // Token embeddings [vocab_size * embed_dim]
let positionEmbeddings = null; // Position embeddings [max_seq_len * embed_dim]
let ggufMetadata = null;     // Store GGUF metadata
let weightsLoaded = false;   // Flag to indicate if real weights are loaded

// GPT-2 model dimensions (will be updated from GGUF metadata)
let GPT2_CONFIG = {
    vocab_size: 50257,
    embed_dim: 768,
    num_heads: 12,
    num_layers: 12,
    ffn_dim: 3072,
    max_seq_len: 1024
};

// =================== MATH HELPERS ===================
function zeros(rows, cols) { return Array(rows).fill(0).map(() => Array(cols).fill(0)); }
function randn(rows, cols, scale=0.1) {
    return Array(rows).fill(0).map(() => Array(cols).fill(0).map(() => (Math.random() - 0.5) * 2 * scale));
}
function softmax(arr) {
    const max = Math.max(...arr);
    const exps = arr.map(x => Math.exp(x - max));
    const sum = exps.reduce((a, b) => a + b, 0);
    return exps.map(x => x / sum);
}
function relu(x) { return Math.max(0, x); }
function gelu(x) {
    return 0.5 * x * (1 + Math.tanh(Math.sqrt(2 / Math.PI) * (x + 0.044715 * x * x * x)));
}
function sigmoid(x) { return 1 / (1 + Math.exp(-x)); }

// =================== TENSOR READING UTILITIES ===================
// GGUF dtype sizes in bytes
const GGUF_DTYPE_SIZE = {
    0: 4,   // F32
    1: 2,   // F16
    2: 4,   // Q4_0 (block size)
    3: 4,   // Q4_1
    6: 4,   // Q5_0
    7: 4,   // Q5_1
    8: 2,   // Q8_0
    9: 2,   // Q8_1
    10: 2,  // Q2_K
    11: 2,  // Q3_K
    12: 2,  // Q4_K
    13: 2,  // Q5_K
    14: 2,  // Q6_K
    15: 2,  // Q8_K
    16: 4,  // I8
    17: 2,  // I16
    18: 4,  // I32
    19: 4,  // F64 (8 bytes but we'll read as f32 pairs)
    20: 2,  // BF16
};

// Read tensor data as Float32Array from GGUF buffer
function readTensorData(tensorInfo) {
    if (!ggufBuffer) return null;
    
    const { name, shape, dtype, dataOffset } = tensorInfo;
    const actualOffset = tensorDataStart + dataOffset;
    
    // Calculate total elements
    let numElements = 1;
    for (const dim of shape) numElements *= dim;
    
    // For F32 (dtype 0), read directly
    if (dtype === 0) {
        const byteLength = numElements * 4;
        if (actualOffset + byteLength > ggufBuffer.byteLength) {
            console.warn(`Tensor ${name} extends beyond buffer`);
            return null;
        }
        return new Float32Array(ggufBuffer, actualOffset, numElements);
    }
    
    // For F16 (dtype 1), convert to F32
    if (dtype === 1) {
        const byteLength = numElements * 2;
        if (actualOffset + byteLength > ggufBuffer.byteLength) {
            console.warn(`Tensor ${name} extends beyond buffer`);
            return null;
        }
        const f16View = new Uint16Array(ggufBuffer, actualOffset, numElements);
        const f32Array = new Float32Array(numElements);
        for (let i = 0; i < numElements; i++) {
            f32Array[i] = float16ToFloat32(f16View[i]);
        }
        return f32Array;
    }
    
    // For BF16 (dtype 20), convert to F32
    if (dtype === 20) {
        const byteLength = numElements * 2;
        if (actualOffset + byteLength > ggufBuffer.byteLength) {
            console.warn(`Tensor ${name} extends beyond buffer`);
            return null;
        }
        const bf16View = new Uint16Array(ggufBuffer, actualOffset, numElements);
        const f32Array = new Float32Array(numElements);
        for (let i = 0; i < numElements; i++) {
            f32Array[i] = bfloat16ToFloat32(bf16View[i]);
        }
        return f32Array;
    }
    
    console.warn(`Unsupported dtype ${dtype} for tensor ${name}, reading as F32`);
    const byteLength = numElements * 4;
    if (actualOffset + byteLength > ggufBuffer.byteLength) {
        return null;
    }
    return new Float32Array(ggufBuffer, actualOffset, numElements);
}

function float16ToFloat32(h) {
    const sign = (h >> 15) & 0x1;
    const exponent = (h >> 10) & 0x1f;
    const mantissa = h & 0x3ff;
    
    if (exponent === 0) {
        if (mantissa === 0) return sign ? -0 : 0;
        // Subnormal
        let e = -14;
        let m = mantissa / 1024;
        while (m < 1) { m *= 2; e--; }
        return (sign ? -1 : 1) * m * Math.pow(2, e);
    }
    if (exponent === 31) {
        return mantissa ? NaN : (sign ? -Infinity : Infinity);
    }
    return (sign ? -1 : 1) * (1 + mantissa / 1024) * Math.pow(2, exponent - 15);
}

function bfloat16ToFloat32(bf16) {
    // BF16 is just the upper 16 bits of F32
    const f32Bits = bf16 << 16;
    const buffer = new ArrayBuffer(4);
    new Uint32Array(buffer)[0] = f32Bits;
    return new Float32Array(buffer)[0];
}

// Reshape 1D Float32Array to 2D JS array
function reshape2D(flat, rows, cols) {
    const result = [];
    for (let i = 0; i < rows; i++) {
        const row = [];
        for (let j = 0; j < cols; j++) {
            row.push(flat[i * cols + j]);
        }
        result.push(row);
    }
    return result;
}

// Extract 1D array from Float32Array
function to1DArray(flat) {
    return Array.from(flat);
}

// =================== TRANSFORMER CLASSES WITH WEIGHT LOADING ===================
class AttentionHead {
    constructor(embedDim, headDim, headIndex, layerWeights) {
        this.embedDim = embedDim;
        this.headDim = headDim;
        
        if (layerWeights && layerWeights.attn_qkv_weight) {
            // Extract this head's portion of the combined QKV weights
            // GPT-2 stores Q, K, V concatenated: [embed_dim, 3 * embed_dim]
            // Each head gets headDim columns from each of Q, K, V sections
            const qkvWeight = layerWeights.attn_qkv_weight; // [embed_dim, 3 * embed_dim]
            const qkvBias = layerWeights.attn_qkv_bias;     // [3 * embed_dim]
            
            const startQ = headIndex * headDim;
            const startK = embedDim + headIndex * headDim;
            const startV = 2 * embedDim + headIndex * headDim;
            
            this.QueryWeights = zeros(embedDim, headDim);
            this.KeyWeights = zeros(embedDim, headDim);
            this.ValueWeights = zeros(embedDim, headDim);
            this.QueryBias = Array(headDim).fill(0);
            this.KeyBias = Array(headDim).fill(0);
            this.ValueBias = Array(headDim).fill(0);
            
            for (let i = 0; i < embedDim; i++) {
                for (let j = 0; j < headDim; j++) {
                    this.QueryWeights[i][j] = qkvWeight[i * 3 * embedDim + startQ + j];
                    this.KeyWeights[i][j] = qkvWeight[i * 3 * embedDim + startK + j];
                    this.ValueWeights[i][j] = qkvWeight[i * 3 * embedDim + startV + j];
                }
            }
            
            if (qkvBias) {
                for (let j = 0; j < headDim; j++) {
                    this.QueryBias[j] = qkvBias[startQ + j];
                    this.KeyBias[j] = qkvBias[startK + j];
                    this.ValueBias[j] = qkvBias[startV + j];
                }
            }
        } else {
            // Random initialization fallback
            this.QueryWeights = randn(embedDim, headDim, Math.sqrt(2/embedDim));
            this.KeyWeights = randn(embedDim, headDim, Math.sqrt(2/embedDim));
            this.ValueWeights = randn(embedDim, headDim, Math.sqrt(2/embedDim));
            this.QueryBias = Array(headDim).fill(0);
            this.KeyBias = Array(headDim).fill(0);
            this.ValueBias = Array(headDim).fill(0);
        }
    }
    
    forward(input, useCausalMask = true) {
        const seqLen = input.length;
        let Q = zeros(seqLen, this.headDim);
        let K = zeros(seqLen, this.headDim);
        let V = zeros(seqLen, this.headDim);
        
        for (let i = 0; i < seqLen; ++i) {
            for (let j = 0; j < this.headDim; ++j) {
                Q[i][j] = this.QueryBias[j];
                K[i][j] = this.KeyBias[j];
                V[i][j] = this.ValueBias[j];
                for (let k = 0; k < this.embedDim; ++k) {
                    Q[i][j] += input[i][k] * this.QueryWeights[k][j];
                    K[i][j] += input[i][k] * this.KeyWeights[k][j];
                    V[i][j] += input[i][k] * this.ValueWeights[k][j];
                }
            }
        }
        
        let scores = zeros(seqLen, seqLen);
        const scale = Math.sqrt(this.headDim);
        
        for (let i = 0; i < seqLen; ++i) {
            for (let j = 0; j < seqLen; ++j) {
                if (useCausalMask && j > i) {
                    scores[i][j] = -Infinity;
                } else {
                    let s = 0;
                    for (let d = 0; d < this.headDim; ++d)
                        s += Q[i][d] * K[j][d];
                    scores[i][j] = s / scale;
                }
            }
            let sm = softmax(scores[i]);
            for (let j = 0; j < seqLen; ++j) scores[i][j] = sm[j];
        }
        
        let out = zeros(seqLen, this.headDim);
        for (let i = 0; i < seqLen; ++i) {
            for (let d = 0; d < this.headDim; ++d) {
                let s = 0;
                for (let k = 0; k < seqLen; ++k)
                    s += scores[i][k] * V[k][d];
                out[i][d] = s;
            }
        }
        return out;
    }
}

class MultiHeadAttention {
    constructor(embedDim, numHeads, layerWeights) {
        this.embedDim = embedDim;
        this.numHeads = numHeads;
        this.headDim = Math.floor(embedDim / numHeads);
        this.heads = [];
        
        for (let i = 0; i < numHeads; ++i) {
            this.heads.push(new AttentionHead(embedDim, this.headDim, i, layerWeights));
        }
        
        if (layerWeights && layerWeights.attn_proj_weight) {
            // Load output projection weights
            const projWeight = layerWeights.attn_proj_weight; // [embed_dim, embed_dim]
            const projBias = layerWeights.attn_proj_bias;     // [embed_dim]
            
            this.OutputWeights = zeros(embedDim, embedDim);
            for (let i = 0; i < embedDim; i++) {
                for (let j = 0; j < embedDim; j++) {
                    this.OutputWeights[i][j] = projWeight[i * embedDim + j];
                }
            }
            this.OutputBias = projBias ? to1DArray(projBias) : Array(embedDim).fill(0);
        } else {
            this.OutputWeights = randn(numHeads * this.headDim, embedDim, Math.sqrt(2/(numHeads*this.headDim)));
            this.OutputBias = Array(embedDim).fill(0);
        }
    }
    
    forward(input) {
        const seqLen = input.length;
        let concat = zeros(seqLen, this.numHeads * this.headDim);
        
        for (let h = 0; h < this.numHeads; ++h) {
            let headOut = this.heads[h].forward(input);
            for (let i = 0; i < seqLen; ++i)
                for (let d = 0; d < this.headDim; ++d)
                    concat[i][h * this.headDim + d] = headOut[i][d];
        }
        
        let out = zeros(seqLen, this.embedDim);
        for (let i = 0; i < seqLen; ++i) {
            for (let j = 0; j < this.embedDim; ++j) {
                let s = this.OutputBias[j];
                for (let k = 0; k < this.numHeads * this.headDim; ++k)
                    s += concat[i][k] * this.OutputWeights[k][j];
                out[i][j] = s;
            }
        }
        return out;
    }
}

class FeedForwardNetwork {
    constructor(embedDim, hiddenDim, layerWeights) {
        if (layerWeights && layerWeights.ffn_up_weight) {
            const upWeight = layerWeights.ffn_up_weight;     // [embed_dim, hidden_dim]
            const upBias = layerWeights.ffn_up_bias;         // [hidden_dim]
            const downWeight = layerWeights.ffn_down_weight; // [hidden_dim, embed_dim]
            const downBias = layerWeights.ffn_down_bias;     // [embed_dim]
            
            this.layer1Weights = zeros(embedDim, hiddenDim);
            for (let i = 0; i < embedDim; i++) {
                for (let j = 0; j < hiddenDim; j++) {
                    this.layer1Weights[i][j] = upWeight[i * hiddenDim + j];
                }
            }
            this.layer1Bias = upBias ? to1DArray(upBias) : Array(hiddenDim).fill(0);
            
            this.layer2Weights = zeros(hiddenDim, embedDim);
            for (let i = 0; i < hiddenDim; i++) {
                for (let j = 0; j < embedDim; j++) {
                    this.layer2Weights[i][j] = downWeight[i * embedDim + j];
                }
            }
            this.layer2Bias = downBias ? to1DArray(downBias) : Array(embedDim).fill(0);
        } else {
            this.layer1Weights = randn(embedDim, hiddenDim, Math.sqrt(2/embedDim));
            this.layer1Bias = Array(hiddenDim).fill(0);
            this.layer2Weights = randn(hiddenDim, embedDim, Math.sqrt(2/hiddenDim));
            this.layer2Bias = Array(embedDim).fill(0);
        }
    }
    
    forward(input) {
        const seqLen = input.length;
        const hiddenDim = this.layer1Bias.length;
        const embedDim = this.layer2Bias.length;
        
        let hidden = zeros(seqLen, hiddenDim);
        for (let i = 0; i < seqLen; ++i) {
            for (let h = 0; h < hiddenDim; ++h) {
                let s = this.layer1Bias[h];
                for (let k = 0; k < input[i].length; ++k)
                    s += input[i][k] * this.layer1Weights[k][h];
                hidden[i][h] = gelu(s); // GPT-2 uses GELU, not ReLU
            }
        }
        
        let out = zeros(seqLen, embedDim);
        for (let i = 0; i < seqLen; ++i) {
            for (let e = 0; e < embedDim; ++e) {
                let s = this.layer2Bias[e];
                for (let h = 0; h < hiddenDim; ++h)
                    s += hidden[i][h] * this.layer2Weights[h][e];
                out[i][e] = s;
            }
        }
        return out;
    }
}

class LayerNorm {
    constructor(embedDim, weights, biases) {
        if (weights && biases) {
            this.gamma = to1DArray(weights);
            this.beta = to1DArray(biases);
        } else {
            this.gamma = Array(embedDim).fill(1.0);
            this.beta = Array(embedDim).fill(0.0);
        }
    }
    
    forward(input) {
        const seqLen = input.length;
        const embedDim = input[0].length;
        let out = zeros(seqLen, embedDim);
        
        for (let i = 0; i < seqLen; ++i) {
            const row = input[i];
            const m = row.reduce((a,b)=>a+b,0)/row.length;
            const variance = row.reduce((a,b)=>a+(b-m)*(b-m),0)/row.length;
            const sd = Math.sqrt(variance + 1e-5);
            for (let j = 0; j < embedDim; ++j)
                out[i][j] = this.gamma[j] * ((row[j] - m) / sd) + this.beta[j];
        }
        return out;
    }
}

class TransformerBlock {
    constructor(embedDim, numHeads, ffnHiddenDim, layerIndex) {
        const layerWeights = getLayerWeights(layerIndex);
        
        this.ln1 = new LayerNorm(embedDim, 
            layerWeights?.ln1_weight, 
            layerWeights?.ln1_bias
        );
        this.attn = new MultiHeadAttention(embedDim, numHeads, layerWeights);
        this.ln2 = new LayerNorm(embedDim,
            layerWeights?.ln2_weight,
            layerWeights?.ln2_bias
        );
        this.ffn = new FeedForwardNetwork(embedDim, ffnHiddenDim, layerWeights);
    }
    
    forward(input) {
        // GPT-2 uses pre-norm architecture
        let ln1Out = this.ln1.forward(input);
        let attnOut = this.attn.forward(ln1Out);
        let res1 = input.map((row, i) => row.map((x, j) => x + attnOut[i][j]));
        
        let ln2Out = this.ln2.forward(res1);
        let ffnOut = this.ffn.forward(ln2Out);
        let res2 = res1.map((row, i) => row.map((x, j) => x + ffnOut[i][j]));
        
        return res2;
    }
}

class Transformer {
    constructor(embedDim, numHeads, numBlocks, ffnHiddenDim, maxSeqLen, numClasses) {
        this.embedDim = embedDim;
        this.numHeads = numHeads;
        this.headDim = Math.floor(embedDim / numHeads);
        this.ffnHiddenDim = ffnHiddenDim;
        this.maxSeqLen = maxSeqLen;
        this.numClasses = numClasses;
        
        this.blocks = [];
        for (let i = 0; i < numBlocks; ++i)
            this.blocks.push(new TransformerBlock(embedDim, numHeads, ffnHiddenDim, i));
        
        // Final layer norm
        const finalLnWeights = getFinalLayerNormWeights();
        this.finalLn = new LayerNorm(embedDim, 
            finalLnWeights?.weight, 
            finalLnWeights?.bias
        );
        
        // Output projection (lm_head) - for language modeling, this is often tied to embeddings
        const lmHeadWeight = getLMHeadWeight();
        if (lmHeadWeight) {
            this.outWeights = zeros(embedDim, numClasses);
            for (let i = 0; i < embedDim; i++) {
                for (let j = 0; j < numClasses; j++) {
                    // lm_head is [vocab_size, embed_dim], we need [embed_dim, vocab_size]
                    this.outWeights[i][j] = lmHeadWeight[j * embedDim + i];
                }
            }
            this.outBias = Array(numClasses).fill(0);
        } else if (embeddingMatrix) {
            // Use tied embeddings (transpose of token embeddings)
            this.outWeights = zeros(embedDim, numClasses);
            for (let i = 0; i < embedDim; i++) {
                for (let j = 0; j < numClasses; j++) {
                    this.outWeights[i][j] = embeddingMatrix[j * embedDim + i];
                }
            }
            this.outBias = Array(numClasses).fill(0);
        } else {
            this.outWeights = randn(embedDim, numClasses, Math.sqrt(2/embedDim));
            this.outBias = Array(numClasses).fill(0);
        }
        
        // Position embeddings
        if (positionEmbeddings) {
            this.positionalEncoding = [];
            for (let pos = 0; pos < Math.min(maxSeqLen, GPT2_CONFIG.max_seq_len); pos++) {
                const posEmbed = [];
                for (let i = 0; i < embedDim; i++) {
                    posEmbed.push(positionEmbeddings[pos * embedDim + i]);
                }
                this.positionalEncoding.push(posEmbed);
            }
        } else {
            this.positionalEncoding = this.createPositionalEncoding(maxSeqLen, embedDim);
        }
    }
    
    createPositionalEncoding(maxSeqLen, embedDim) {
        let PE = zeros(maxSeqLen, embedDim);
        for (let pos = 0; pos < maxSeqLen; ++pos) {
            for (let i = 0; i < embedDim; ++i) {
                const angle = pos / Math.pow(10000, (2 * Math.floor(i / 2)) / embedDim);
                PE[pos][i] = (i % 2 === 0) ? Math.sin(angle) : Math.cos(angle);
            }
        }
        return PE;
    }
    
    predict(sequenceTokens) {
        let seqLen = sequenceTokens.length;
        let input = zeros(seqLen, this.embedDim);
        
        for (let i = 0; i < seqLen; ++i)
            for (let j = 0; j < this.embedDim; ++j)
                input[i][j] = sequenceTokens[i][j] + this.positionalEncoding[i][j];
        
        for (let blk of this.blocks)
            input = blk.forward(input);
        
        // Apply final layer norm
        input = this.finalLn.forward(input);
        
        // For autoregressive generation, we only need the last position's logits
        const lastPos = input[seqLen - 1];
        
        let out = Array(this.numClasses).fill(0);
        for (let c = 0; c < this.numClasses; ++c) {
            let s = this.outBias[c];
            for (let j = 0; j < this.embedDim; ++j)
                s += lastPos[j] * this.outWeights[j][c];
            out[c] = s; // Raw logits for softmax sampling
        }
        return out;
    }
}

// =================== WEIGHT EXTRACTION HELPERS ===================
// GPT-2 GGUF tensor name mapping
const GPT2_TENSOR_NAMES = {
    token_embed: 'token_embd.weight',
    position_embed: 'position_embd.weight',
    final_ln_weight: 'output_norm.weight',
    final_ln_bias: 'output_norm.bias',
    lm_head: 'output.weight',
    
    // Per-layer patterns (replace {n} with layer index)
    ln1_weight: 'blk.{n}.attn_norm.weight',
    ln1_bias: 'blk.{n}.attn_norm.bias',
    attn_qkv_weight: 'blk.{n}.attn_qkv.weight',
    attn_qkv_bias: 'blk.{n}.attn_qkv.bias',
    attn_proj_weight: 'blk.{n}.attn_output.weight',
    attn_proj_bias: 'blk.{n}.attn_output.bias',
    ln2_weight: 'blk.{n}.ffn_norm.weight',
    ln2_bias: 'blk.{n}.ffn_norm.bias',
    ffn_up_weight: 'blk.{n}.ffn_up.weight',
    ffn_up_bias: 'blk.{n}.ffn_up.bias',
    ffn_down_weight: 'blk.{n}.ffn_down.weight',
    ffn_down_bias: 'blk.{n}.ffn_down.bias',
};

function getLayerWeights(layerIndex) {
    if (!weightsLoaded) return null;
    
    const get = (pattern) => {
        const name = pattern.replace('{n}', layerIndex);
        const tensor = ggufTensors[name];
        if (tensor && tensor.data) return tensor.data;
        return null;
    };
    
    return {
        ln1_weight: get(GPT2_TENSOR_NAMES.ln1_weight),
        ln1_bias: get(GPT2_TENSOR_NAMES.ln1_bias),
        attn_qkv_weight: get(GPT2_TENSOR_NAMES.attn_qkv_weight),
        attn_qkv_bias: get(GPT2_TENSOR_NAMES.attn_qkv_bias),
        attn_proj_weight: get(GPT2_TENSOR_NAMES.attn_proj_weight),
        attn_proj_bias: get(GPT2_TENSOR_NAMES.attn_proj_bias),
        ln2_weight: get(GPT2_TENSOR_NAMES.ln2_weight),
        ln2_bias: get(GPT2_TENSOR_NAMES.ln2_bias),
        ffn_up_weight: get(GPT2_TENSOR_NAMES.ffn_up_weight),
        ffn_up_bias: get(GPT2_TENSOR_NAMES.ffn_up_bias),
        ffn_down_weight: get(GPT2_TENSOR_NAMES.ffn_down_weight),
        ffn_down_bias: get(GPT2_TENSOR_NAMES.ffn_down_bias),
    };
}

function getFinalLayerNormWeights() {
    if (!weightsLoaded) return null;
    const weightTensor = ggufTensors[GPT2_TENSOR_NAMES.final_ln_weight];
    const biasTensor = ggufTensors[GPT2_TENSOR_NAMES.final_ln_bias];
    return {
        weight: weightTensor?.data || null,
        bias: biasTensor?.data || null
    };
}

function getLMHeadWeight() {
    if (!weightsLoaded) return null;
    const tensor = ggufTensors[GPT2_TENSOR_NAMES.lm_head];
    return tensor?.data || null;
}

// =================== GGUF PARSER ===================
function readUint32(view, offset) {
    if (offset + 4 > view.byteLength) throw new Error("readUint32: offset out of bounds");
    return view.getUint32(offset, true);
}

function readUint64(view, offset) {
    if (offset + 8 > view.byteLength) throw new Error("readUint64: offset out of bounds");
    const low = view.getUint32(offset, true);
    const high = view.getUint32(offset + 4, true);
    return low + high * 0x100000000;
}

function readStringSafe(view, offset) {
    if (offset + 8 > view.byteLength) {
        return { string: "<incomplete string header>", nextOffset: view.byteLength, truncated: true };
    }
    let strLen;
    try { 
        strLen = readUint64(view, offset); 
    } catch (e) {
        return { string: `<error: ${e.message}>`, nextOffset: view.byteLength, truncated: true };
    }
    const stringOffset = offset + 8;
    if (stringOffset + strLen > view.byteLength) {
        return {
            string: `<string truncated (${strLen} bytes, got only ${view.byteLength - stringOffset})>`,
            nextOffset: view.byteLength,
            truncated: true
        };
    }
    const bytes = new Uint8Array(view.buffer, view.byteOffset + stringOffset, strLen);
    const string = new TextDecoder().decode(bytes);
    return { string, nextOffset: stringOffset + strLen, truncated: false };
}

const GGUF_TYPE = {
    UINT8: 0,  INT8: 1, UINT16: 2, INT16: 3,
    UINT32: 4, INT32: 5, FLOAT32: 6, BOOL: 7,
    STRING: 8, ARRAY: 9, UINT64: 10, INT64: 11, FLOAT64: 12
};

const GGUF_DTYPE_STR = {
    0: 'F32', 1: 'F16', 2: 'Q4_0', 3: 'Q4_1', 4: 'Q4_2', 5: 'Q4_3',
    6: 'Q5_0', 7: 'Q5_1', 8: 'Q8_0', 9: 'Q8_1', 10: 'Q2_K', 11: 'Q3_K',
    12: 'Q4_K', 13: 'Q5_K', 14: 'Q6_K', 15: 'Q8_K', 16: 'I8', 17: 'I16',
    18: 'I32', 19: 'F64', 20: 'BF16'
};

function readMetadataValue(view, offset, type) {
    let value, nextOffset = offset;
    const checkBounds = needed => {
        if (offset + needed > view.byteLength)
            throw new Error(`Not enough data (need ${needed} bytes at offset ${offset}, has ${view.byteLength - offset})`);
    };
    try {
        switch(type) {
        case GGUF_TYPE.UINT8:
            checkBounds(1);
            value = view.getUint8(offset);
            nextOffset = offset + 1; break;
        case GGUF_TYPE.INT8:
            checkBounds(1);
            value = view.getInt8(offset);
            nextOffset = offset + 1; break;
        case GGUF_TYPE.UINT16:
            checkBounds(2);
            value = view.getUint16(offset, true);
            nextOffset = offset + 2; break;
        case GGUF_TYPE.INT16:
            checkBounds(2);
            value = view.getInt16(offset, true);
            nextOffset = offset + 2; break;
        case GGUF_TYPE.UINT32:
            checkBounds(4);
            value = readUint32(view, offset);
            nextOffset = offset + 4; break;
        case GGUF_TYPE.INT32:
            checkBounds(4);
            value = view.getInt32(offset, true);
            nextOffset = offset + 4; break;
        case GGUF_TYPE.FLOAT32:
            checkBounds(4);
            value = view.getFloat32(offset, true);
            nextOffset = offset + 4; break;
        case GGUF_TYPE.BOOL:
            checkBounds(1);
            value = view.getUint8(offset) !== 0;
            nextOffset = offset + 1; break;
        case GGUF_TYPE.STRING: {
            const result = readStringSafe(view, offset);
            value = result.string;
            nextOffset = result.nextOffset;
            break;
        }
        case GGUF_TYPE.UINT64:
            checkBounds(8);
            value = readUint64(view, offset);
            nextOffset = offset + 8; break;
        case GGUF_TYPE.ARRAY:
            checkBounds(12);
            const arrayType = readUint32(view, offset);
            const arrayCount = readUint64(view, offset + 4);
            nextOffset = offset + 12;
            let arrayValues = [];
            for (let i = 0; i < arrayCount; i++) {
                const elemResult = readMetadataValue(view, nextOffset, arrayType);
                arrayValues.push(elemResult.value);
                nextOffset = elemResult.nextOffset;
            }
            value = arrayValues;
            break;
        default:
            value = 'UNSUPPORTED';
            nextOffset = offset;
        }
    } catch(e) {
        value = `<error: ${e.message}>`;
        nextOffset = view.byteLength;
    }
    return { value, nextOffset };
}

function parseAndLoadGguf(buffer) {
    ggufBuffer = buffer;
    let view = new DataView(buffer);
    let offset = 0;
    let out = "";
    
    try {
        let magicBytes = [];
        for (let i = 0; i < 4; ++i) magicBytes.push(view.getUint8(offset + i));
        let magic = String.fromCharCode(...magicBytes);
        let version = view.getUint32(4, true);
        offset += 8;
        let tensorCount = readUint64(view, offset); offset += 8;
        let metadataCount = readUint64(view, offset); offset += 8;
        
        out += `<div>Magic: <b>${magic}</b> | Version: <b>${version}</b></div>`;
        out += `<div>Tensors: <b>${tensorCount}</b> | Metadata: <b>${metadataCount}</b></div>`;
        
        // Parse metadata
        ggufMetadata = {};
        for (let i = 0; i < metadataCount; ++i) {
            const { string: key, nextOffset: afterKey, truncated } = readStringSafe(view, offset);
            if (truncated) break;
            offset = afterKey;
            if (offset + 4 > view.byteLength) break;
            const valueType = readUint32(view, offset); offset += 4;
            const { value, nextOffset: afterValue } = readMetadataValue(view, offset, valueType);
            offset = afterValue;
            ggufMetadata[key] = value;
        }
        
        // Update GPT-2 config from metadata
        if (ggufMetadata['gpt2.embedding_length']) GPT2_CONFIG.embed_dim = ggufMetadata['gpt2.embedding_length'];
        if (ggufMetadata['gpt2.block_count']) GPT2_CONFIG.num_layers = ggufMetadata['gpt2.block_count'];
        if (ggufMetadata['gpt2.attention.head_count']) GPT2_CONFIG.num_heads = ggufMetadata['gpt2.attention.head_count'];
        if (ggufMetadata['gpt2.feed_forward_length']) GPT2_CONFIG.ffn_dim = ggufMetadata['gpt2.feed_forward_length'];
        if (ggufMetadata['gpt2.context_length']) GPT2_CONFIG.max_seq_len = ggufMetadata['gpt2.context_length'];
        
        out += `<div style='color:#4f4;margin-top:8px'><b>Model Config:</b> embed_dim=${GPT2_CONFIG.embed_dim}, layers=${GPT2_CONFIG.num_layers}, heads=${GPT2_CONFIG.num_heads}</div>`;
        
        // Parse tensor info
        let tensorListOut = "";
        let tensors = [];
        
        for (let i = 0; i < tensorCount; ++i) {
            if (offset >= view.byteLength) break;
            
            const { string: name, nextOffset: afterName, truncated } = readStringSafe(view, offset);
            if (truncated) break;
            offset = afterName;
            
            if (offset + 4 > view.byteLength) break;
            let n_dims = readUint32(view, offset); offset += 4;
            
            let shape = [];
            for (let d = 0; d < n_dims; ++d) {
                if (offset + 8 > view.byteLength) break;
                shape.push(readUint64(view, offset)); offset += 8;
            }
            
            if (offset + 4 > view.byteLength) break;
            let dtype = readUint32(view, offset); offset += 4;
            
            if (offset + 8 > view.byteLength) break;
            let dataOffset = readUint64(view, offset); offset += 8;
            
            let dtypeHuman = GGUF_DTYPE_STR[dtype] || `dtype_${dtype}`;
            tensors.push({ name, shape, dtype, dataOffset });
            tensorListOut += `<div>${name} | [${shape.join(",")}] | ${dtypeHuman}</div>\n`;
        }
        
        // Calculate tensor data start (aligned to 32 bytes)
        const alignment = 32;
        tensorDataStart = offset;
        let remainder = tensorDataStart % alignment;
        if (remainder !== 0) {
            tensorDataStart += alignment - remainder;
        }
        
        console.log('Tensor data starts at:', tensorDataStart);
        
        // Extract ALL tensor data
        let extractedCount = 0;
        let failedCount = 0;
        
        for (const tensor of tensors) {
            const data = readTensorData(tensor);
            if (data) {
                ggufTensors[tensor.name] = {
                    ...tensor,
                    data: data
                };
                extractedCount++;
            } else {
                failedCount++;
            }
        }
        
        // Extract special tensors
        const tokenEmbedTensor = ggufTensors[GPT2_TENSOR_NAMES.token_embed];
        if (tokenEmbedTensor) {
            embeddingMatrix = tokenEmbedTensor.data;
            out += `<div style='color:#4f4'>‚úì Token embeddings: [${tokenEmbedTensor.shape.join(',')}]</div>`;
        }
        
        const posEmbedTensor = ggufTensors[GPT2_TENSOR_NAMES.position_embed];
        if (posEmbedTensor) {
            positionEmbeddings = posEmbedTensor.data;
            out += `<div style='color:#4f4'>‚úì Position embeddings: [${posEmbedTensor.shape.join(',')}]</div>`;
        }
        
        weightsLoaded = extractedCount > 10; // Consider loaded if we got at least 10 tensors
        
        out += `<div style='margin-top:12px'>`;
        out += `<span style='color:#4f4'>‚úì Extracted: ${extractedCount} tensors</span>`;
        if (failedCount > 0) out += ` | <span style='color:#f84'>Failed: ${failedCount}</span>`;
        out += `</div>`;
        
        // Show weight loading status
        const statusDiv = document.getElementById('weightsStatus');
        if (weightsLoaded) {
            statusDiv.innerHTML = `<div style='color:#4f4; font-size:1.1em; margin-top:12px'>
                <b>‚úì ALL ${extractedCount} WEIGHTS LOADED!</b><br>
                <span style='font-size:0.9em'>Ready to generate with real GPT-2 weights.</span>
            </div>`;
        } else {
            statusDiv.innerHTML = `<div style='color:#f84'>‚ö† Only ${extractedCount} tensors extracted. Model may not work correctly.</div>`;
        }
        
        out += `<div class="tensorList"><b>All Tensors (${tensors.length}):</b><br>${tensorListOut}</div>`;
        document.getElementById("tensorListPanel").innerHTML = out;
        document.getElementById('ggufStatus').textContent = `Loaded ${tensorCount} tensors`;
        
    } catch (e) {
        document.getElementById('ggufStatus').textContent = "Error: " + e.message;
        console.error('GGUF parse error:', e);
    }
}

document.getElementById('loadGgufBtn').onclick = function() {
    const file = document.getElementById('ggufFileInput').files[0];
    if (!file) {
        document.getElementById('ggufStatus').textContent = "No file selected!";
        return;
    }
    document.getElementById('ggufStatus').textContent = "Reading file...";
    document.getElementById('tensorListPanel').innerHTML = "";
    document.getElementById('weightsStatus').innerHTML = "";
    
    file.arrayBuffer().then(buf => {
        parseAndLoadGguf(buf);
    });
};

function runTransformer() {
    const seqLen = parseInt(document.getElementById('seqLen').value);
    const embedDim = parseInt(document.getElementById('embedDim').value);
    const numHeads = parseInt(document.getElementById('numHeads').value);
    const numBlocks = parseInt(document.getElementById('numBlocks').value);
    const ffnHidden = parseInt(document.getElementById('ffnHidden').value);
    const numClasses = parseInt(document.getElementById('numClasses').value);

    let userVecField = document.getElementById('inputVec');
    let vec = userVecField.value.split(",").map(x => parseFloat(x.trim())).filter(x => !isNaN(x));
    if (vec.length !== embedDim) {
        document.getElementById('output').innerHTML = `<span style="color:#f885;">ERROR:</span> Your input vector must have exactly ${embedDim} values.`;
        return;
    }

    let seqTokens = [vec];
    for (let i = 1; i < seqLen; ++i) {
        seqTokens.push(Array(embedDim).fill(0).map(() => Math.random()));
    }
    
    let transformer = new Transformer(embedDim, numHeads, numBlocks, ffnHidden, Math.max(seqLen, 50), numClasses);
    const prediction = transformer.predict(seqTokens);
    
    document.getElementById('output').innerHTML =
        `<b>Input Sequence:</b> First vector = [${vec.map(x=>x.toFixed(4)).join(", ")}] ...<br>
         <b>Prediction:</b> [${prediction.slice(0, 20).map(x => x.toFixed(4)).join(', ')}${prediction.length > 20 ? '...' : ''}]<br>
         <b>Weights:</b> ${weightsLoaded ? '‚úì Using GGUF weights' : '‚ö† Using random weights'}`;
}
</script>
<script type="module">
import { AutoTokenizer } from "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.13.0/dist/transformers.min.js";

let localTokenizer = null;
let tokenizerContent = null;

const originalFetch = window.fetch;
window.fetch = function(url, options) {
  const urlStr = typeof url === 'string' ? url : url.toString();
  
  if (tokenizerContent && urlStr.includes('tokenizer.json')) {
    return Promise.resolve(new Response(tokenizerContent, {
      status: 200,
      headers: { 'Content-Type': 'application/json' }
    }));
  }
  
  if (urlStr.includes('tokenizer_config.json')) {
    return Promise.resolve(new Response(JSON.stringify({}), {
      status: 200,
      headers: { 'Content-Type': 'application/json' }
    }));
  }
  
  return originalFetch(url, options);
};

window.loadLocalTokenizer = async function() {
    const fileInput = document.getElementById("localTokenizerInput");
    if (!fileInput.files.length) {
        document.getElementById("localTokOutput").textContent = "No file selected!";
        return;
    }

    try {
        let file = fileInput.files[0];
        let text = await file.text();
        JSON.parse(text);
        tokenizerContent = text;
        localTokenizer = await AutoTokenizer.from_pretrained('local-model');
        document.getElementById("localTokOutput").textContent = "‚úì Tokenizer loaded!";
    } catch(e) {
        document.getElementById("localTokOutput").textContent = "Failed: " + e.message;
        console.error("Failed to load tokenizer:", e);
    }
};

window.encodeTextLocal = async function() {
    if (!localTokenizer) {
        document.getElementById("tokencodeOutput").innerHTML = "<span style='color:#f44'>Load tokenizer first!</span>";
        return;
    }
    const text = document.getElementById('userText').value;
    try {
        const encoded = await localTokenizer.encode(text);
        const decoded = await localTokenizer.decode(encoded);
        document.getElementById('tokencodeOutput').innerHTML =
            `<b>Token IDs:</b> ${JSON.stringify(encoded)}<br><b>Decoded:</b> "${decoded}"`;
    } catch(e) {
        document.getElementById('tokencodeOutput').innerHTML =
            `<span style='color:#f44'>Error: ${e.message}</span>`;
    }
}

let generationCancelled = false;

window.cancelGeneration = function() {
    generationCancelled = true;
};

window.generateText = async function() {
    const output = document.getElementById('generationOutput');
    
    if (!localTokenizer) {
        output.innerHTML = "<span style='color:#f44'>‚ùå Load tokenizer first!</span>";
        return;
    }
    
    if (!embeddingMatrix) {
        output.innerHTML = "<span style='color:#f44'>‚ùå Load GGUF file first!</span>";
        return;
    }
    
    const promptText = document.getElementById('promptText').value;
    const maxTokens = parseInt(document.getElementById('maxTokens').value);
    
    if (!promptText.trim()) {
        output.innerHTML = "<span style='color:#f44'>‚ùå Enter a prompt!</span>";
        return;
    }
    
    generationCancelled = false;
    
    try {
        output.innerHTML = "<div style='color:#4af'>‚è≥ Encoding prompt...</div>";
        
        let currentIds = await localTokenizer.encode(promptText);
        console.log('Input token IDs:', currentIds);
        
        const vocabSize = GPT2_CONFIG.vocab_size;
        const embedDim = GPT2_CONFIG.embed_dim;
        const numHeads = GPT2_CONFIG.num_heads;
        const numBlocks = GPT2_CONFIG.num_layers;
        const ffnHidden = GPT2_CONFIG.ffn_dim;
        
        output.innerHTML = `<div style='color:#4af'>‚è≥ Building transformer (${numBlocks} layers, ${numHeads} heads)...</div>`;
        await new Promise(resolve => setTimeout(resolve, 50));
        
        let transformer = new Transformer(embedDim, numHeads, numBlocks, ffnHidden, 1024, vocabSize);
        
        const usingRealWeights = weightsLoaded;
        output.innerHTML = `<div style='color:#4f4'>‚úì Ready! ${usingRealWeights ? 'Using REAL GPT-2 weights' : 'Using random weights'}<br>
            <button class='btn' onclick='cancelGeneration()' style='margin-top:8px'>Cancel</button></div>`;
        await new Promise(resolve => setTimeout(resolve, 100));
        
        let generatedTokens = [];
        let startTime = Date.now();
        
        for (let step = 0; step < maxTokens; step++) {
            if (generationCancelled) {
                output.innerHTML += "<div style='color:#f84'>‚ùå Cancelled</div>";
                break;
            }
            
            const stepStart = Date.now();
            output.innerHTML = `<div style='color:#4af'>‚è≥ Token ${step + 1}/${maxTokens}... <button class='btn' onclick='cancelGeneration()'>Cancel</button></div>`;
            await new Promise(resolve => setTimeout(resolve, 10));
            
            let inputEmbeddings = [];
            for (let tokenId of currentIds) {
                let embedding = [];
                const startIdx = tokenId * embedDim;
                for (let i = 0; i < embedDim; i++) {
                    embedding.push(embeddingMatrix[startIdx + i]);
                }
                inputEmbeddings.push(embedding);
            }
            
            const logits = transformer.predict(inputEmbeddings);
            
            // Temperature sampling (greedy for now)
            let bestTokenId = 0;
            let bestLogit = logits[0];
            for (let i = 1; i < logits.length; i++) {
                if (logits[i] > bestLogit) {
                    bestLogit = logits[i];
                    bestTokenId = i;
                }
            }
            
            const stepTime = Date.now() - stepStart;
            console.log(`Step ${step}: token ${bestTokenId} in ${stepTime}ms`);
            
            currentIds = [...currentIds, bestTokenId];
            generatedTokens.push(bestTokenId);
            
            const partialText = await localTokenizer.decode(currentIds);
            output.innerHTML = `
                <div style='color:#4af'>‚è≥ Token ${step + 1}/${maxTokens} (${stepTime}ms) <button class='btn' onclick='cancelGeneration()'>Cancel</button></div>
                <div style='margin-top:8px; padding:8px; background:#2a2a2a; border-left: 3px solid #4af;'>
                    <b>Current:</b> "${partialText}"
                </div>
            `;
            
            if (bestTokenId === 50256) break; // EOS token
        }
        
        const totalTime = ((Date.now() - startTime) / 1000).toFixed(1);
        const generatedText = await localTokenizer.decode(currentIds);
        
        output.innerHTML = `
            <div style='color:#4f4'><b>‚úì Complete in ${totalTime}s!</b></div>
            <div style='margin-top:8px'><b>Prompt:</b> "${promptText}"</div>
            <div style='margin-top:8px; padding:12px; background:#2a2a2a; border-left: 3px solid #4af;'>
                <b>Generated text:</b><br>
                <span style='font-size:1.1em'>"${generatedText}"</span>
            </div>
            <div style='color:#aaa; font-size:0.9em; margin-top:8px'>
                ${generatedTokens.length} tokens | ${(generatedTokens.length / parseFloat(totalTime)).toFixed(2)} tokens/sec
            </div>
            <div style='color:#888; font-size:0.85em; margin-top:4px'>
                ${usingRealWeights ? '‚úì Used REAL GPT-2 weights from GGUF' : '‚ö† Used random weights (load GGUF for real output)'}
            </div>
        `;
        
    } catch(e) {
        output.innerHTML = `<span style='color:#f44'>‚ùå Error: ${e.message}</span>`;
        console.error("Generation error:", e);
        console.error(e.stack);
    }
}
</script>
</body>
</html>
