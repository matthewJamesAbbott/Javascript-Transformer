<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Vanilla JS Transformer Demo</title>
    <style>
        body { font-family: monospace; background: #232323; color: #ddd; padding: 24px; }
        .output { margin-top: 24px; font-size: 1.2em; }
        .btn { background: #444; color: #fff; border: none; padding: 8px 16px; cursor: pointer; margin-top: 12px;}
        .btn:hover { background: #666; }
        input[type="number"] { width: 60px; }
    </style>
</head>
<body>
    <h1>JavaScript Transformer (Vanilla, in-browser)</h1>
    <div>
        <label>Sequence length: <input type="number" id="seqLen" value="10" min="1" max="64"></label><br>
        <label>Embedding dimension: <input type="number" id="embedDim" value="16" min="4" max="128"></label><br>
        <label>Num heads: <input type="number" id="numHeads" value="2" min="1" max="8"></label><br>
        <label>Blocks: <input type="number" id="numBlocks" value="2" min="1" max="8"></label><br>
        <label>Hidden (FFN) dim: <input type="number" id="ffnHidden" value="32" min="4" max="256"></label><br>
        <label>Classes: <input type="number" id="numClasses" value="3" min="1" max="10"></label>
    </div>
    <button class="btn" onclick="runTransformer()">Run Transformer Prediction</button>
    <div class="output" id="output"></div>
    <script>
        // ===== Helper Math Functions =====
        function zeros(rows, cols) {
            return Array(rows).fill(0).map(() => Array(cols).fill(0));
        }
        function randn(rows, cols, scale=0.1) {
            return Array(rows).fill(0).map(() => Array(cols).fill(0).map(() => (Math.random() - 0.5) * 2 * scale));
        }
        function softmax(arr) {
            // 1D array
            const max = Math.max(...arr);
            const exps = arr.map(x => Math.exp(x - max));
            const sum = exps.reduce((a, b) => a + b, 0);
            return exps.map(x => x / sum);
        }
        function relu(x) { return Math.max(0, x); }
        function sigmoid(x) { return 1 / (1 + Math.exp(-x)); }
        function mean(arr) { return arr.reduce((a, b) => a + b, 0) / arr.length; }
        function stdev(arr) {
            const m = mean(arr);
            return Math.sqrt(mean(arr.map(x => (x - m) ** 2)) + 1e-6);
        }
        function clone2D(arr) {
            return arr.map(row => row.slice());
        }
        // ===== Data Structures =====
        class AttentionHead {
            constructor(embedDim, headDim) {
                this.embedDim = embedDim;
                this.headDim = headDim;
                this.QueryWeights = randn(embedDim, headDim, Math.sqrt(2/embedDim));
                this.KeyWeights   = randn(embedDim, headDim, Math.sqrt(2/embedDim));
                this.ValueWeights = randn(embedDim, headDim, Math.sqrt(2/embedDim));
                this.QueryBias = Array(headDim).fill(0);
                this.KeyBias   = Array(headDim).fill(0);
                this.ValueBias = Array(headDim).fill(0);
            }
            forward(input) { // input [seqLen][embedDim]
                const seqLen = input.length;
                // Q, K, V: [seqLen][headDim]
                let Q = zeros(seqLen, this.headDim), K = zeros(seqLen, this.headDim), V = zeros(seqLen, this.headDim);
                for (let i = 0; i < seqLen; ++i) {
                    for (let j = 0; j < this.headDim; ++j) {
                        Q[i][j] = this.QueryBias[j];
                        K[i][j] = this.KeyBias[j];
                        V[i][j] = this.ValueBias[j];
                        for (let k = 0; k < this.embedDim; ++k) {
                            Q[i][j] += input[i][k] * this.QueryWeights[k][j];
                            K[i][j] += input[i][k] * this.KeyWeights[k][j];
                            V[i][j] += input[i][k] * this.ValueWeights[k][j];
                        }
                    }
                }
                // Attention scores [seqLen][seqLen]
                let scores = zeros(seqLen, seqLen);
                const scale = Math.sqrt(this.headDim);
                for (let i = 0; i < seqLen; ++i) {
                    for (let j = 0; j < seqLen; ++j) {
                        let s = 0;
                        for (let d = 0; d < this.headDim; ++d)
                            s += Q[i][d] * K[j][d];
                        scores[i][j] = s / scale;
                    }
                    // Softmax over scores for position i
                    let sm = softmax(scores[i]);
                    for (let j = 0; j < seqLen; ++j) scores[i][j] = sm[j];
                }
                // Output: [seqLen][headDim]
                let out = zeros(seqLen, this.headDim);
                for (let i = 0; i < seqLen; ++i) {
                    for (let d = 0; d < this.headDim; ++d) {
                        let s = 0;
                        for (let k = 0; k < seqLen; ++k)
                            s += scores[i][k] * V[k][d];
                        out[i][d] = s;
                    }
                }
                return out;
            }
        }
        class MultiHeadAttention {
            constructor(embedDim, numHeads) {
                this.embedDim = embedDim;
                this.numHeads = numHeads;
                this.headDim = Math.floor(embedDim / numHeads);
                this.heads = [];
                for (let i = 0; i < numHeads; ++i)
                    this.heads.push(new AttentionHead(embedDim, this.headDim));
                // Output projection
                this.OutputWeights = randn(numHeads * this.headDim, embedDim, Math.sqrt(2/(numHeads*this.headDim)));
                this.OutputBias = Array(embedDim).fill(0);
            }
            forward(input) { // input: [seqLen][embedDim]
                const seqLen = input.length;
                let concat = zeros(seqLen, this.numHeads * this.headDim);
                // forward each head
                for (let h = 0; h < this.numHeads; ++h) {
                    let headOut = this.heads[h].forward(input);
                    for (let i = 0; i < seqLen; ++i)
                        for (let d = 0; d < this.headDim; ++d)
                            concat[i][h * this.headDim + d] = headOut[i][d];
                }
                // Project to embedDim
                let out = zeros(seqLen, this.embedDim);
                for (let i = 0; i < seqLen; ++i) {
                    for (let j = 0; j < this.embedDim; ++j) {
                        let s = this.OutputBias[j];
                        for (let k = 0; k < this.numHeads * this.headDim; ++k)
                            s += concat[i][k] * this.OutputWeights[k][j];
                        out[i][j] = s;
                    }
                }
                return out;
            }
        }
        class FeedForwardNetwork {
            constructor(embedDim, hiddenDim) {
                // layer 1: embedDim -> hiddenDim
                this.layer1Weights = randn(embedDim, hiddenDim, Math.sqrt(2/embedDim));
                this.layer1Bias = Array(hiddenDim).fill(0);
                // layer 2: hiddenDim -> embedDim
                this.layer2Weights = randn(hiddenDim, embedDim, Math.sqrt(2/hiddenDim));
                this.layer2Bias = Array(embedDim).fill(0);
            }
            forward(input) { // input: [seqLen][embedDim]
                const seqLen = input.length;
                let hidden = zeros(seqLen, this.layer1Bias.length);
                for (let i = 0; i < seqLen; ++i) {
                    for (let h = 0; h < this.layer1Bias.length; ++h) {
                        let s = this.layer1Bias[h];
                        for (let k = 0; k < input[i].length; ++k)
                            s += input[i][k] * this.layer1Weights[k][h];
                        hidden[i][h] = relu(s);
                    }
                }
                // 2nd layer
                let out = zeros(seqLen, this.layer2Bias.length);
                for (let i = 0; i < seqLen; ++i) {
                    for (let e = 0; e < this.layer2Bias.length; ++e) {
                        let s = this.layer2Bias[e];
                        for (let h = 0; h < hidden[i].length; ++h)
                            s += hidden[i][h] * this.layer2Weights[h][e];
                        out[i][e] = s;
                    }
                }
                return out;
            }
        }
        class LayerNorm {
            constructor(embedDim) {
                this.gamma = Array(embedDim).fill(1.0);
                this.beta  = Array(embedDim).fill(0.0);
            }
            forward(input) { // input [seqLen][embedDim]
                const seqLen = input.length;
                const embedDim = input[0].length;
                let out = zeros(seqLen, embedDim);
                for (let i = 0; i < seqLen; ++i) {
                    const row = input[i];
                    const m = mean(row);
                    const sd = stdev(row);
                    for (let j = 0; j < embedDim; ++j)
                        out[i][j] = this.gamma[j] * ((row[j] - m) / sd) + this.beta[j];
                }
                return out;
            }
        }
        class TransformerBlock {
            constructor(embedDim, numHeads, ffnHiddenDim) {
                this.attn = new MultiHeadAttention(embedDim, numHeads);
                this.ln1 = new LayerNorm(embedDim);
                this.ffn = new FeedForwardNetwork(embedDim, ffnHiddenDim);
                this.ln2 = new LayerNorm(embedDim);
            }
            forward(input) {
                let attnOut = this.attn.forward(input);
                let res1 = input.map((row, i) => row.map((x, j) => x + attnOut[i][j]));
                let ln1Out = this.ln1.forward(res1);
                let ffnOut = this.ffn.forward(ln1Out);
                let res2 = ln1Out.map((row, i) => row.map((x, j) => x + ffnOut[i][j]));
                let ln2Out = this.ln2.forward(res2);
                return ln2Out;
            }
        }
        class Transformer {
            constructor(embedDim, numHeads, numBlocks, ffnHiddenDim, maxSeqLen, numClasses) {
                this.embedDim = embedDim;
                this.numHeads = numHeads;
                this.headDim = Math.floor(embedDim / numHeads);
                this.ffnHiddenDim = ffnHiddenDim;
                this.maxSeqLen = maxSeqLen;
                this.numClasses = numClasses;
                this.blocks = [];
                for (let i = 0; i < numBlocks; ++i)
                    this.blocks.push(new TransformerBlock(embedDim, numHeads, ffnHiddenDim));
                // positional enc
                this.positionalEncoding = this.createPositionalEncoding(maxSeqLen, embedDim);
                // output layer
                this.outWeights = randn(embedDim, numClasses, Math.sqrt(2/embedDim));
                this.outBias = Array(numClasses).fill(0);
            }
            createPositionalEncoding(maxSeqLen, embedDim) {
                let PE = zeros(maxSeqLen, embedDim);
                for (let pos = 0; pos < maxSeqLen; ++pos) {
                    for (let i = 0; i < embedDim; ++i) {
                        const angle = pos / Math.pow(10000, (2 * Math.floor(i / 2)) / embedDim);
                        PE[pos][i] = (i % 2 === 0) ? Math.sin(angle) : Math.cos(angle);
                    }
                }
                return PE;
            }
            predict(sequenceTokens) {
                // sequenceTokens: [seqLen][embedDim]
                let seqLen = sequenceTokens.length;
                // Add positional encoding
                let input = zeros(seqLen, this.embedDim);
                for (let i = 0; i < seqLen; ++i)
                    for (let j = 0; j < this.embedDim; ++j)
                        input[i][j] = sequenceTokens[i][j] + this.positionalEncoding[i][j];
                // Transformer blocks
                for (let blk of this.blocks)
                    input = blk.forward(input);
                // Global average pooling
                let pooled = Array(this.embedDim).fill(0);
                for (let j = 0; j < this.embedDim; ++j) {
                    let s = 0;
                    for (let i = 0; i < seqLen; ++i)
                        s += input[i][j];
                    pooled[j] = s / seqLen;
                }
                // Output
                let out = Array(this.numClasses).fill(0);
                for (let c = 0; c < this.numClasses; ++c) {
                    let s = this.outBias[c];
                    for (let j = 0; j < this.embedDim; ++j)
                        s += pooled[j] * this.outWeights[j][c];
                    out[c] = sigmoid(s); // or softmax for multiclass: softmax(out)[c]
                }
                return out;
            }
        }
        // ======= Demo Runner =======
        function runTransformer() {
            // Get UI params
            const seqLen = parseInt(document.getElementById('seqLen').value);
            const embedDim = parseInt(document.getElementById('embedDim').value);
            const numHeads = parseInt(document.getElementById('numHeads').value);
            const numBlocks = parseInt(document.getElementById('numBlocks').value);
            const ffnHidden = parseInt(document.getElementById('ffnHidden').value);
            const numClasses = parseInt(document.getElementById('numClasses').value);
            // Generate sequence
            let seqTokens = [];
            for (let i = 0; i < seqLen; ++i) {
                seqTokens.push(Array(embedDim).fill(0).map(() => Math.random()));
            }
            let transformer = new Transformer(embedDim, numHeads, numBlocks, ffnHidden, Math.max(seqLen, 50), numClasses);
            const prediction = transformer.predict(seqTokens);
            // Output
            document.getElementById('output').innerHTML =
                `<b>Input Sequence:</b> [${seqLen} x ${embedDim} random tensor]<br>
                 <b>Prediction:</b> [${prediction.map(x => x.toFixed(4)).join(', ')}]`;
        }
    </script>
</body>
</html>
